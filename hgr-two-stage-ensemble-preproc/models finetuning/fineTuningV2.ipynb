{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition CNN Fine-tuning on Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive\n",
    "This cell mounts your Google Drive to the Colab environment, allowing access to datasets and saving models. You'll be prompted for authorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os.path as osp\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# setting random number seed.\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "Set your training parameters here. These were previously handled by `argparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your Configuration ---\n",
    "# TODO: Adjust MODEL_NAME and other parameters as needed.\n",
    "MODEL_NAME = \"vgg16\"  # Choices: \"vgg16\", \"vgg19\", \"mobilenet\", \"mobilenet_v2\"\n",
    "NUM_CLASSES = 14\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10 # Adjust as needed for Colab training times\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# In the configuration section (cell 3):\n",
    "DRIVE_PROJECT_ROOT = \"/content/drive/MyDrive\"\n",
    "DATASET_ROOT_PATH = os.path.join(DRIVE_PROJECT_ROOT, \"data\")\n",
    "\n",
    "\n",
    "# Path for saving checkpoints and metrics\n",
    "CHECKPOINTS_BASE_PATH = osp.join(DRIVE_PROJECT_ROOT, \"checkpoints\") \n",
    "\n",
    "# Path for saving the final model\n",
    "FINAL_MODEL_SAVE_PATH_BASE = DRIVE_PROJECT_ROOT\n",
    "\n",
    "# Example image path (update if your example image is located elsewhere or named differently)\n",
    "# Make sure this example image exists if you want the visualization to run.\n",
    "EXAMPLE_IMAGE_PATH = osp.join(DATASET_ROOT_PATH, \"train/Gesture_0/example_gesture.jpg\") # Adjust if needed\n",
    "\n",
    "print(f\"Using Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset root path: {DATASET_ROOT_PATH}\")\n",
    "print(f\"Checkpoints will be saved in subfolders of: {CHECKPOINTS_BASE_PATH}\")\n",
    "print(f\"Final model will be saved in: {FINAL_MODEL_SAVE_PATH_BASE}\")\n",
    "print(f\"Example image path: {EXAMPLE_IMAGE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CUDA Availability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.device_count() > 0:\n",
    "        print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Image Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform:\n",
    "    def __init__(self, size, mean, std):\n",
    "        self.data_transform = {\n",
    "            \"train\": transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomResizedCrop(size, scale=(0.5, 1.0)),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std),\n",
    "                ]\n",
    "            ),\n",
    "            \"val\": transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(size),\n",
    "                    transforms.CenterCrop(size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean, std),\n",
    "                ]\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def __call__(self, img, phase=\"train\"):\n",
    "        return self.data_transform[phase](img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Image Visualization (Optional)\n",
    "This cell visualizes an example image and its transformed version.\n",
    "Ensure `EXAMPLE_IMAGE_PATH` is set correctly in the configuration cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_path = EXAMPLE_IMAGE_PATH \n",
    "\n",
    "try:\n",
    "    img_originalsize = Image.open(image_file_path)\n",
    "    img_display = img_originalsize.resize((256, 256)) \n",
    "    img_display = img_display.convert(\"RGB\") \n",
    "    plt.imshow(img_display)\n",
    "    plt.title(\"Example Gesture Image (Original-like)\")\n",
    "    plt.show()\n",
    "\n",
    "    size = 256\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    transform = ImageTransform(size, mean, std)\n",
    "\n",
    "    img_to_transform = Image.open(image_file_path).convert(\"RGB\")\n",
    "    img_transformed_display = transform(img_to_transform, phase=\"train\")\n",
    "    print(f\"Transformed image shape: {img_transformed_display.shape}\")\n",
    "\n",
    "    img_transformed_display_np = img_transformed_display.numpy().transpose((1, 2, 0))\n",
    "    img_transformed_display_np = np.clip(img_transformed_display_np, 0, 1)\n",
    "    plt.imshow(img_transformed_display_np)\n",
    "    plt.title(\"Example Gesture Image (Transformed)\")\n",
    "    plt.show()\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Warning: Example image for display not found at {image_file_path}. Skipping display.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during example image display: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Path List and Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(phase=\"train\"):\n",
    "    # Uses DATASET_ROOT_PATH from the configuration cell\n",
    "    rootpath = DATASET_ROOT_PATH \n",
    "    target_path = osp.join(\n",
    "        rootpath, phase, \"Gesture_*\", \"*.jpg\"\n",
    "    ) \n",
    "    print(f\"Searching for images in: {target_path}\")\n",
    "    path_list = glob.glob(target_path, recursive=False)\n",
    "    if not path_list:\n",
    "        print(\n",
    "            f\"Warning: No images found for phase '{phase}' with pattern '{target_path}'. Check your dataset structure and path.\"\n",
    "        )\n",
    "    return path_list\n",
    "\n",
    "train_list = make_datapath_list(phase=\"train\")\n",
    "print(f\"Number of training gesture images: {len(train_list)}\")\n",
    "\n",
    "val_list = make_datapath_list(phase=\"val\")\n",
    "print(f\"Number of validation gesture images: {len(val_list)}\")\n",
    "\n",
    "\n",
    "class GestureDataset(data.Dataset):\n",
    "    def __init__(self, file_list, transform=None, phase=\"train\"):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_transformed = self.transform(img, self.phase)\n",
    "        label_name = img_path.split(osp.sep)[-2]\n",
    "        try:\n",
    "            label = int(label_name.replace(\"Gesture_\", \"\"))\n",
    "        except ValueError:\n",
    "            print(\n",
    "                f\"Error parsing label from folder name: {label_name} in path {img_path}\"\n",
    "            )\n",
    "            label = -1 \n",
    "        return img_transformed, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 256\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = GestureDataset(\n",
    "    file_list=train_list, transform=ImageTransform(size, mean, std), phase=\"train\"\n",
    ")\n",
    "val_dataset = GestureDataset(\n",
    "    file_list=val_list, transform=ImageTransform(size, mean, std), phase=\"val\"\n",
    ")\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    raise ValueError(\n",
    "        \"Training dataset is empty. Please check your 'data/train' folder (inside DRIVE_PROJECT_ROOT) and `make_datapath_list` function.\"\n",
    "    )\n",
    "if len(val_dataset) == 0:\n",
    "    print(\n",
    "        \"Warning: Validation dataset is empty. Training will proceed without validation if this is intended.\"\n",
    "    )\n",
    "\n",
    "# Use BATCH_SIZE from configuration\n",
    "# Consider reducing num_workers if you encounter issues in Colab (e.g., to 2)\n",
    "num_workers_colab = 2 # Often 2 is a safe bet for Colab free tier\n",
    "print(f\"Using num_workers: {num_workers_colab} for DataLoaders\")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers_colab, pin_memory=True\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers_colab, pin_memory=True\n",
    ")\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Definition and Fine-tuning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained = True\n",
    "\n",
    "if MODEL_NAME == \"vgg16\":\n",
    "    net = models.vgg16(\n",
    "        weights=models.VGG16_Weights.IMAGENET1K_V1 if use_pretrained else None\n",
    "    )\n",
    "    net.classifier[6] = nn.Linear(in_features=4096, out_features=NUM_CLASSES)\n",
    "elif MODEL_NAME == \"vgg19\":\n",
    "    net = models.vgg19(\n",
    "        weights=models.VGG19_Weights.IMAGENET1K_V1 if use_pretrained else None\n",
    "    )\n",
    "    net.classifier[6] = nn.Linear(in_features=4096, out_features=NUM_CLASSES)\n",
    "elif MODEL_NAME == \"mobilenet\": # Note: Original script used mobilenet_v3_large\n",
    "    net = models.mobilenet_v3_large(\n",
    "        weights=(\n",
    "            models.MobileNet_V3_Large_Weights.IMAGENET1K_V1 if use_pretrained else None\n",
    "        )\n",
    "    )\n",
    "    net.classifier[3] = nn.Linear(\n",
    "        in_features=net.classifier[3].in_features, out_features=NUM_CLASSES\n",
    "    )\n",
    "elif MODEL_NAME == \"mobilenet_v2\":\n",
    "    net = models.mobilenet_v2(\n",
    "        weights=models.MobileNet_V2_Weights.IMAGENET1K_V1 if use_pretrained else None\n",
    "    )\n",
    "    if hasattr(net.classifier, \"1\") and isinstance(net.classifier[1], nn.Linear):\n",
    "        net.classifier[1] = nn.Linear(net.classifier[1].in_features, NUM_CLASSES)\n",
    "    elif isinstance(net.classifier, nn.Sequential) and isinstance(net.classifier[-1], nn.Linear):\n",
    "        last_layer_in_features = net.classifier[-1].in_features\n",
    "        net.classifier[-1] = nn.Linear(last_layer_in_features, NUM_CLASSES)\n",
    "    elif isinstance(net.classifier, nn.Linear):\n",
    "         net.classifier = nn.Linear(net.classifier.in_features, NUM_CLASSES)\n",
    "    else:\n",
    "        print(\"Warning: MobileNetV2 classifier structure not standard. Attempting replacement using net.last_channel.\")\n",
    "        if hasattr(net, \"last_channel\"):\n",
    "            net.classifier = nn.Linear(net.last_channel, NUM_CLASSES)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"Cannot automatically determine input features for MobileNetV2 classifier. Please check model structure.\"\n",
    "            )\n",
    "else:\n",
    "    raise ValueError(f\"Model {MODEL_NAME} not supported.\")\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "# net.train() # This will be set in the training loop\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "params_to_update = []\n",
    "\n",
    "if MODEL_NAME in [\"vgg16\", \"vgg19\"]:\n",
    "    params_to_update_1 = []\n",
    "    params_to_update_2 = []\n",
    "    params_to_update_3 = []\n",
    "\n",
    "    update_param_names_1 = [\"features\"]\n",
    "    update_param_names_2 = [\n",
    "        \"classifier.0.weight\", \"classifier.0.bias\",\n",
    "        \"classifier.3.weight\", \"classifier.3.bias\",\n",
    "    ]\n",
    "    update_param_names_3 = [\"classifier.6.weight\", \"classifier.6.bias\"]\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        if update_param_names_1[0] in name: # Fine-tune deeper layers less\n",
    "            param.requires_grad = True\n",
    "            params_to_update_1.append(param)\n",
    "        elif name in update_param_names_2:\n",
    "            param.requires_grad = True\n",
    "            params_to_update_2.append(param)\n",
    "        elif name in update_param_names_3: # Train classifier head more\n",
    "            param.requires_grad = True\n",
    "            params_to_update_3.append(param)\n",
    "    \n",
    "    optimizer = optim.SGD([\n",
    "        {'params': params_to_update_1, 'lr': LEARNING_RATE / 10}, # Slower LR for features\n",
    "        {'params': params_to_update_2, 'lr': LEARNING_RATE / 2},  # Medium LR \n",
    "        {'params': params_to_update_3, 'lr': LEARNING_RATE}      # Higher LR for new classifier\n",
    "    ], momentum=0.9)\n",
    "\n",
    "else:  # For mobilenet, mobilenet_v2 - fine-tune only the new classifier\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if MODEL_NAME == \"mobilenet\": # mobilenet_v3_large\n",
    "        for param in net.classifier[3].parameters():\n",
    "            param.requires_grad = True\n",
    "            params_to_update.append(param)\n",
    "    elif MODEL_NAME == \"mobilenet_v2\":\n",
    "        final_classifier_layer = net.classifier[-1] if isinstance(net.classifier, nn.Sequential) else net.classifier\n",
    "        for param in final_classifier_layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            params_to_update.append(param)\n",
    "    \n",
    "    print(f\"Optimizing {len(params_to_update)} parameters for {MODEL_NAME} (classifier only).\")\n",
    "    optimizer = optim.SGD(params_to_update, lr=LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, model_name_arg, dataloaders_dict_arg, criterion_arg, optimizer_arg, num_epochs_arg):\n",
    "    train_accuracy_list = []\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    val_loss_list = []\n",
    "    metrics_history = []\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on device: {device}\")\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "    net.to(device)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # Use CHECKPOINTS_BASE_PATH from configuration for saving\n",
    "    model_save_dir = osp.join(CHECKPOINTS_BASE_PATH, f\"checkpoints_{model_name_arg}\")\n",
    "    if not osp.exists(model_save_dir):\n",
    "        os.makedirs(model_save_dir)\n",
    "        print(f\"Created directory for model checkpoints: {model_save_dir}\")\n",
    "\n",
    "    metrics_file_path = osp.join(model_save_dir, f\"training_metrics_{model_name_arg}.csv\")\n",
    "\n",
    "    for epoch in range(num_epochs_arg):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs_arg}\")\n",
    "        print(\"-------------\")\n",
    "        epoch_metrics = {\"epoch\": epoch + 1}\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                net.train()\n",
    "            else:\n",
    "                if not dataloaders_dict_arg[\"val\"].dataset or len(dataloaders_dict_arg[\"val\"].dataset) == 0:\n",
    "                    print(\"Validation dataset is empty or not found, skipping validation phase.\")\n",
    "                    epoch_metrics[\"val_loss\"] = None\n",
    "                    epoch_metrics[\"val_acc\"] = None\n",
    "                    if phase == \"val\" and \"val\" not in val_accuracy_list: # Ensure lists are extendable\n",
    "                         val_loss_list.append(float('nan'))\n",
    "                         val_accuracy_list.append(float('nan'))\n",
    "                    continue\n",
    "                net.eval()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "\n",
    "            if not dataloaders_dict_arg[phase].dataset or len(dataloaders_dict_arg[phase].dataset) == 0:\n",
    "                print(f\"Dataset for phase '{phase}' is empty, skipping.\")\n",
    "                if phase == \"train\":\n",
    "                    epoch_metrics[\"train_loss\"] = None\n",
    "                    epoch_metrics[\"train_acc\"] = None\n",
    "                    train_loss_list.append(float('nan'))\n",
    "                    train_accuracy_list.append(float('nan'))\n",
    "                elif phase == \"val\": # This case is covered above, but for safety\n",
    "                    epoch_metrics[\"val_loss\"] = None\n",
    "                    epoch_metrics[\"val_acc\"] = None\n",
    "                    val_loss_list.append(float('nan'))\n",
    "                    val_accuracy_list.append(float('nan'))\n",
    "                continue\n",
    "            \n",
    "            current_dataloader = dataloaders_dict_arg[phase]\n",
    "            current_dataset_size = len(current_dataloader.dataset)\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(current_dataloader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer_arg.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion_arg(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer_arg.step()\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = epoch_loss / current_dataset_size\n",
    "            epoch_acc = epoch_corrects.double() / current_dataset_size\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_loss_list.append(epoch_loss)\n",
    "                train_accuracy_list.append(epoch_acc.cpu().item())\n",
    "                epoch_metrics[\"train_loss\"] = epoch_loss\n",
    "                epoch_metrics[\"train_acc\"] = epoch_acc.cpu().item()\n",
    "            elif phase == \"val\":\n",
    "                val_loss_list.append(epoch_loss)\n",
    "                val_accuracy_list.append(epoch_acc.cpu().item())\n",
    "                epoch_metrics[\"val_loss\"] = epoch_loss\n",
    "                epoch_metrics[\"val_acc\"] = epoch_acc.cpu().item()\n",
    "        \n",
    "        checkpoint_name = f\"gesture_{model_name_arg}_epoch_{epoch+1}.pth\"\n",
    "        checkpoint_path = osp.join(model_save_dir, checkpoint_name)\n",
    "        torch.save(net.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved model checkpoint to {checkpoint_path}\")\n",
    "\n",
    "        metrics_history.append(epoch_metrics)\n",
    "        try:\n",
    "            df_metrics = pd.DataFrame(metrics_history)\n",
    "            df_metrics.to_csv(metrics_file_path, index=False)\n",
    "            print(f\"Updated metrics saved to {metrics_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save metrics to CSV: {e}\")\n",
    "\n",
    "\n",
    "    return train_accuracy_list, train_loss_list, val_accuracy_list, val_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using configuration variables\n",
    "train_acc_list, train_loss_list, val_acc_list, val_loss_list = train_model(\n",
    "    net,\n",
    "    MODEL_NAME,\n",
    "    dataloaders_dict,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs_arg=NUM_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FINAL_MODEL_SAVE_PATH_BASE from configuration\n",
    "final_save_path = osp.join(FINAL_MODEL_SAVE_PATH_BASE, f\"gesture_{MODEL_NAME}_finetuned_final.pth\")\n",
    "try:\n",
    "    torch.save(net.state_dict(), final_save_path)\n",
    "    print(f\"Final model saved to {final_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving final model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if val_acc_list and val_loss_list have valid (non-NaN) data before plotting\n",
    "valid_val_acc = [x for x in val_acc_list if not np.isnan(x)]\n",
    "valid_val_loss = [x for x in val_loss_list if not np.isnan(x)]\n",
    "\n",
    "# Ensure train lists also have data\n",
    "valid_train_acc = [x for x in train_acc_list if not np.isnan(x)]\n",
    "valid_train_loss = [x for x in train_loss_list if not np.isnan(x)]\n",
    "\n",
    "\n",
    "if valid_val_acc and valid_val_loss and valid_train_acc and valid_train_loss:\n",
    "    # Determine the number of epochs plotted based on the shortest list that's not empty\n",
    "    # This handles cases where validation might have been skipped for some epochs or altogether.\n",
    "    num_epochs_plotted = min(len(valid_train_acc) if valid_train_acc else float('inf'), \n",
    "                             len(valid_train_loss) if valid_train_loss else float('inf'),\n",
    "                             len(valid_val_acc) if valid_val_acc else float('inf'),\n",
    "                             len(valid_val_loss) if valid_val_loss else float('inf'))\n",
    "\n",
    "    if num_epochs_plotted == float('inf') or num_epochs_plotted == 0 :\n",
    "        print(\"Not enough data to plot results.\")\n",
    "    else:\n",
    "        epoch_plot_range = list(range(1, num_epochs_plotted + 1))\n",
    "        \n",
    "        fig, ax = plt.subplots(facecolor=\"w\", figsize=(12, 6))\n",
    "        \n",
    "        # Plot training data up to num_epochs_plotted\n",
    "        if valid_train_acc: ax.plot(epoch_plot_range, train_acc_list[:num_epochs_plotted], label=\"Training Accuracy\", marker='o')\n",
    "        if valid_train_loss: ax.plot(epoch_plot_range, train_loss_list[:num_epochs_plotted], label=\"Training Loss\", marker='o')\n",
    "        \n",
    "        # Plot validation data up to num_epochs_plotted\n",
    "        if valid_val_acc: ax.plot(epoch_plot_range, val_acc_list[:num_epochs_plotted], label=\"Validation Accuracy\", marker='x')\n",
    "        if valid_val_loss: ax.plot(epoch_plot_range, val_loss_list[:num_epochs_plotted], label=\"Validation Loss\", marker='x')\n",
    "        \n",
    "        if epoch_plot_range:\n",
    "            plt.xticks(epoch_plot_range)\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.set_title(f\"Training and Validation Metrics for {MODEL_NAME} ({num_epochs_plotted} Epochs)\")\n",
    "        ax.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save the plot to Drive\n",
    "        plot_save_path = osp.join(CHECKPOINTS_BASE_PATH, f\"checkpoints_{MODEL_NAME}\", f\"training_plot_{MODEL_NAME}.png\")\n",
    "        try:\n",
    "            plt.savefig(plot_save_path)\n",
    "            print(f\"Plot saved to {plot_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save plot: {e}\")\n",
    "        plt.show()\n",
    "\n",
    "elif valid_train_acc and valid_train_loss:\n",
    "    # Only training data is available\n",
    "    num_epochs_plotted = min(len(valid_train_acc), len(valid_train_loss))\n",
    "    if num_epochs_plotted > 0:\n",
    "        epoch_plot_range = list(range(1, num_epochs_plotted + 1))\n",
    "        fig, ax = plt.subplots(facecolor=\"w\", figsize=(12, 6))\n",
    "        ax.plot(epoch_plot_range, train_acc_list[:num_epochs_plotted], label=\"Training Accuracy\", marker='o')\n",
    "        ax.plot(epoch_plot_range, train_loss_list[:num_epochs_plotted], label=\"Training Loss\", marker='o')\n",
    "        if epoch_plot_range:\n",
    "            plt.xticks(epoch_plot_range)\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.set_title(f\"Training Metrics for {MODEL_NAME} ({num_epochs_plotted} Epochs)\")\n",
    "        ax.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plot_save_path = osp.join(CHECKPOINTS_BASE_PATH, f\"checkpoints_{MODEL_NAME}\", f\"training_plot_{MODEL_NAME}.png\")\n",
    "        try:\n",
    "            plt.savefig(plot_save_path)\n",
    "            print(f\"Plot saved to {plot_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save plot: {e}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid training data to plot.\")\n",
    "else:\n",
    "    print(\"No validation data to plot or validation was skipped. Only training data might be available if training occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "End of Notebook. Remember to adjust paths in the \"Configuration\" cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
